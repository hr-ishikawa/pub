# **Regression**
https://pycaret.readthedocs.io/en/latest/api/regression.html  

## **setup**
|Param|Desc|説明|
|-----|----|----|
|data: pandas.DataFrame|Shape (n_samples, n_features), where n_samples is the number of samples and n_features is the number of features.|Shape (n_samples, n_features), n_samplesはサンプル数、n_featuresは特徴量の数です。|
|target: str|Name of the target column to be passed in as a string. The target variable can be either binary or multiclass.|文字列で渡されるターゲット列の名前。対象となる変数は，バイナリまたはマルチクラスのいずれかである。|
|train_size: float,<br>default = 0.7|Proportion of the dataset to be used for training and validation. Should be between 0.0 and 1.0.|トレーニングと検証に使用するデータセットの割合。0.0から1.0の間でなければならない。|
|test_data: pandas.DataFrame,<br>default = None|If not None, test_data is used as a hold-out set and train_size parameter is ignored. test_data must be labelled and the shape of data and test_data must match.|Noneでない場合、test_dataはホールドアウトセットとして使用され、train_sizeパラメータは無視される。test_dataはラベル付けされていなければならず、dataとtest_dataの形状は一致していなければならない。|
|preprocess: bool,<br>default = True|When set to False, no transformations are applied except for train_test_split and custom transformations passed in custom_pipeline param. Data must be ready for modeling (no missing values, no dates, categorical data encoding), when preprocess is set to False.|Falseに設定すると，train_test_splitとcustom_pipelineパラメータで渡されたカスタム変換を除いて，変換は行われない。PreprocessがFalseに設定されている場合，データはモデル化の準備ができていなければならない（欠損値なし，日付なし，カテゴリーデータのエンコーディング）。|
|imputation_type: str,<br>default = 'simple'|The type of imputation to use. Can be either 'simple' or 'iterative'.|使用する入力のタイプです。simple」または「iterative」のいずれかです。|
|iterative_imputation_iters: int,<br>default = 5|Number of iterations. Ignored when imputation_type is not 'iterative'.|反復処理の回数。imputation_typeが'iterative'でない場合は無視されます。|
|categorical_features: list of str,<br>default = None|If the inferred data types are not correct or the silent param is set to True, categorical_features param can be used to overwrite or define the data types. It takes a list of strings with column names that are categorical.|推論されたデータ型が正しくない場合や，silentパラメータがTrueに設定されている場合，categorical_featuresパラメータを用いてデータ型を上書きまたは定義することができます。これは，カテゴライズされたカラム名を持つ文字列のリストを受け取ります。|
|categorical_imputation: str,<br>default = 'constant'| Missing values in categorical features are imputed with a constant 'not_available' value. The other available option is 'mode'.|categorical特徴の欠損値は、定数の'not_available'値で入力されます。他に利用可能なオプションは'mode'です。|
|categorical_iterative_imputer: str,<br>default = 'lightgbm'|Estimator for iterative imputation of missing values in categorical features. Ignored when imputation_type is not 'iterative'.|カテゴリ特徴量の欠損値を反復的に入力するための推定量です。imputation_typeが'iterative'でない場合は無視されます。|
|ordinal_features: dict,<br>default = None|Encode categorical features as ordinal. For example, a categorical feature with 'low', 'medium', 'high' values where low < medium < high can be passed as ordinal_features = { 'column_name' : ['low', 'medium', 'high'] }.|カテゴリー特徴を順序でエンコードします。例えば、'low', 'medium', 'high'の値を持つカテゴリー特徴で、low < medium < highの場合、ordinal_features = { 'column_name' : ['low', 'medium', 'high'] }と渡すことができます。|
|high_cardinality_features: list of str,<br>default = None|When categorical features contains many levels, it can be compressed into fewer levels using this parameter. It takes a list of strings with column names that are categorical.|カテゴリ特徴が多くのレベルを含んでいる場合、このパラメータを使って、より少ないレベルに圧縮することができます。このパラメータはカテゴライズされたカラム名を持つ文字列のリストを取ります。|
|high_cardinality_method: str,<br>default = 'frequency'|Categorical features with high cardinality are replaced with the frequency of values in each level occurring in the training dataset. Other available method is 'clustering' which trains the K-Means clustering algorithm on the statistical attribute of the training data and replaces the original value of feature with the cluster label. The number of clusters is determined by optimizing Calinski-Harabasz and Silhouette criterion.|高いカーディナリティを持つカテゴリー特徴は、各レベルの値がトレーニングデータセットに出現する頻度に置き換えられます。その他の方法として、「clustering」があります。これは、学習データの統計的属性に基づいてK-Meansクラスタリングアルゴリズムを学習し、特徴の元の値をクラスタラベルに置き換えます。クラスタの数は，Calinski-Harabasz基準とSilhouette基準を最適化して決定されます。|
|numeric_features: list of str,<br>default = None| If the inferred data types are not correct or the silent param is set to True, numeric_features param can be used to overwrite or define the data types. It takes a list of strings with column names that are numeric.| 推論されたデータ型が正しくない場合，あるいは silent パラメーターが True に設定されている場合， numeric_features パラメーターを用いてデータ型を上書きあるいは定義することができます．これは，数値を表す列名を持つ文字列のリストを受け取ります。|
|numeric_imputation: str,<br>default = 'mean'|Missing values in numeric features are imputed with 'mean' value of the feature in the training dataset. The other available option is 'median' or 'zero'.|数値特徴の欠損値は、トレーニングデータセットにおけるその特徴の'mean'値で入力されます。その他のオプションとして、'median'または'zero'があります。|
|numeric_iterative_imputer: str,<br>default = 'lightgbm'|Estimator for iterative imputation of missing values in numeric features. Ignored when imputation_type is set to 'simple'.|数値特徴量の欠損値を反復的に入力するための推定量です。imputation_typeが'simple'に設定されている場合は無視されます。|
|date_features: list of str,<br>default = None|If the inferred data types are not correct or the silent param is set to True, date_features param can be used to overwrite or define the data types. It takes a list of strings with column names that are DateTime.|推論されたデータ型が正しくない場合、あるいは silent パラメーターが True に設定されている場合、 date_features パラメーターを用いてデータ型を上書きあるいは定義することができます。これは、DateTimeのカラム名を持つ文字列のリストを受け取ります。|
|ignore_features: list of str,<br>default = None|ignore_features param can be used to ignore features during model training. It takes a list of strings with column names that are to be ignored.|ignore_features パラメーターは，モデル学習時に特徴量を無視するために利用されます．これは，無視されるべき列名を持つ文字列のリストを受け取ります．|
|normalize: bool,<br>default = False|When set to True, it transforms the numeric features by scaling them to a given range. Type of scaling is defined by the normalize_method parameter.| Trueに設定すると，数値特徴量を指定された範囲にスケーリングして変換します．スケーリングのタイプはnormalize_methodパラメータで定義される。|
|normalize_method: str,<br>default = 'zscore'|Defines the method for scaling. By default, normalize method is set to 'zscore' The standard zscore is calculated as z = (x - u) / s. Ignored when normalize is not True. The other options are:<br>        minmax: scales and translates each feature individually such that it is in the range of 0 - 1.<br>        maxabs: scales and translates each feature individually such that the maximal absolute value of each feature will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.<br>        robust: scales and translates each feature according to the Interquartile range. When the dataset contains outliers, robust scaler often gives better results.|スケーリングの方法を定義します。標準のzscoreはz = (x - u) / sとして計算される。normalizeがTrueでない場合は無視されます。その他のオプションは次のとおりです。<br>        minmax：各特徴を0〜1の範囲に収まるように個別にスケーリング、変換します。<br>        maxabs：各特徴の絶対値の最大値が 1.0 になるように，各特徴を個別にスケーリング，変換します．データのシフトやセンタリングは行わないので、スパース性は失われません。<br>        robust: 各特徴を四分位範囲に応じてスケーリング、変換します。データセットに外れ値が含まれている場合は，robust scalerの方が良い結果が得られることが多い。|
|transformation: bool,<br>default = False|When set to True, it applies the power transform to make data more Gaussian-like. Type of transformation is defined by the transformation_method parameter.|Trueに設定すると、データをよりガウス型に近づけるために、power transformationを適用する。変換のタイプはtransformation_methodパラメータで定義される。|
|transformation_method: str,<br>default = 'yeo-johnson'|Defines the method for transformation. By default, the transformation method is set to 'yeo-johnson'. The other available option for transformation is 'quantile'. Ignored when transformation is not True.|変換の方法を定義します。デフォルトでは、変換方法は'yeo-johnson'に設定されています。その他の変換方法として'quantile'があります。変換がTrueでない場合は無視されます。|
|handle_unknown_categorical: bool,<br>default = True|When set to True, unknown categorical levels in unseen data are replaced by the most or least frequent level as learned in the training dataset.|Trueに設定すると、見たことのないデータの未知のカテゴリーレベルは、トレーニングデータセットで学習された最も頻度の高いレベルまたは最も頻度の低いレベルで置き換えられる。|
|unknown_categorical_method: str,<br>default = 'least_frequent'|Method used to replace unknown categorical levels in unseen data. Method can be set to 'least_frequent' or 'most_frequent'.|未認識データの未知のカテゴリーレベルを置き換えるために使用される方法。メソッドは'lest_frequent'または'most_frequent'に設定できる。|
|pca: bool,<br>default = False|When set to True, dimensionality reduction is applied to project the data into a lower dimensional space using the method defined in pca_method parameter.|Trueに設定すると、pca_methodパラメータで定義された方法を用いて、データをより低次元の空間に投影するために次元削減が適用される。|
|pca_method: str,<br>default = 'linear'|The 'linear' method performs uses Singular Value Decomposition. Other options are:<br>        kernel: dimensionality reduction through the use of RBF kernel.<br>        incremental: replacement for 'linear' pca when the dataset is too large.| linear "メソッドは特異値分解を行う。その他のオプションは以下の通りです。<br>        kernel: RBFカーネルを使って次元削減を行う。<br>        incremental: データセットが大きすぎる場合に、「linear」pcaの代わりに使用する。|
|pca_components: int or float,<br>default = None|Number of components to keep. if pca_components is a float, it is treated as a target percentage for information retention. When pca_components is an integer it is treated as the number of features to be kept. pca_components must be less than the original number of features. Ignored when pca is not True.|pca_componentsがfloatの場合は，情報保持の目標割合として扱われる．pca_componentsが整数の場合は、保持する特徴の数として扱われます。pca_componentsは元の特徴の数よりも少なくなければなりません。pca が True でない場合は無視されます。|
|ignore_low_variance: bool,<br>default = False|When set to True, all categorical features with insignificant variances are removed from the data. The variance is calculated using the ratio of unique values to the number of samples, and the ratio of the most common value to the frequency of the second most common value.|Trueに設定すると、有意でない分散を持つ全てのカテゴリー特徴がデータから削除される。分散は、ユニークな値のサンプル数に対する比率と、最も一般的な値の2番目に一般的な値の頻度に対する比率を用いて計算される。|
|combine_rare_levels: bool,<br>default = False|When set to True, frequency percentile for levels in categorical features below a certain threshold is combined into a single level.|rueに設定すると、ある閾値以下のカテゴリー特徴におけるレベルの頻度パーセンタイルが1つのレベルにまとめられます。|
|rare_level_threshold: float,<br>default = 0.1|Percentile distribution below which rare categories are combined. Ignored when combine_rare_levels is not True.|希少なカテゴリが結合される閾値以下のパーセンタイル分布。combine_rare_levelsがTrueでない場合は無視されます。|
|bin_numeric_features: list of str,<br>default = None|To convert numeric features into categorical, bin_numeric_features parameter can be used. It takes a list of strings with column names to be discretized. It does so by using 'sturges' rule to determine the number of clusters and then apply KMeans algorithm. Original values of the feature are then replaced by the cluster label.|数値特徴をカテゴリーに変換するために、bin_numeric_featuresパラメータを使用できます。bin_numeric_featuresパラメータは、離散化されるカラム名を持つ文字列のリストを受け取ります。これは、「sturges」ルールを使用してクラスタの数を決定し、KMeansアルゴリズムを適用することで行われます。そして、その特徴の元の値は、クラスタラベルに置き換えられます。|
|remove_outliers: bool,<br>default = False|When set to True, outliers from the training data are removed using the Singular Value Decomposition.|Trueに設定すると、Singular Value Decompositionを用いてトレーニングデータから外れ値を除去する。|
|outliers_threshold: float,<br>default = 0.05|The percentage outliers to be removed from the training dataset. Ignored when remove_outliers is not True.|トレーニングデータから削除される外れ値の割合です。remove_outliersがTrueでない場合は無視されます。|
|remove_multicollinearity: bool,<br>default = False|When set to True, features with the inter-correlations higher than the defined threshold are removed. When two features are highly correlated with each other, the feature that is less correlated with the target variable is removed. Only considers numeric features.|Trueに設定すると、相互相関が定義された閾値よりも高い特徴が削除される。2つの特徴が互いに高い相関を持つ場合、ターゲット変数との相関が低い特徴が削除される。数値特徴のみを考慮します。|
|multicollinearity_threshold: float,<br>default = 0.9|Threshold for correlated features. Ignored when remove_multicollinearity is not True.|相関性のある特徴のしきい値。remove_multicollinearityがTrueでない場合は無視されます。|
|remove_perfect_collinearity: bool,<br>default = True|When set to True, perfect collinearity (features with correlation = 1) is removed from the dataset, when two features are 100% correlated, one of it is randomly removed from the dataset.|Trueに設定すると、完全な共線性（相関が1のフィーチャー）がデータセットから削除される。2つのフィーチャーが100％相関している場合、そのうちの1つがデータセットからランダムに削除される。|
|create_clusters: bool,<br>default = False|When set to True, an additional feature is created in training dataset where each instance is assigned to a cluster. The number of clusters is determined by optimizing Calinski-Harabasz and Silhouette criterion.|Trueに設定すると、訓練データセットに追加の特徴が作成され、各インスタンスがクラスターに割り当てられる。クラスタの数はCalinski-Harabasz基準とSilhouette基準の最適化により決定される。|
|cluster_iter: int,<br>default = 20|Number of iterations for creating cluster. Each iteration represents cluster size. Ignored when create_clusters is not True.|クラスタを生成するための反復回数。各反復はクラスタサイズを表します。create_clustersがTrueでない場合は無視されます。|
|polynomial_features: bool,<br>default = False|When set to True, new features are derived using existing numeric features.|Trueに設定された場合、新しい特徴は既存の数値特徴を使って導き出される。|
|polynomial_degree: int,<br>default = 2|Degree of polynomial features. For example, if an input sample is two dimensional and of the form [a, b], the polynomial features with degree = 2 are: [1, a, b, a^2, ab, b^2]. Ignored when polynomial_features is not True.|多項式特徴量の次数。例えば、入力サンプルが2次元で、[a, b]の形をしている場合、次数=2の多項式特徴量は [1, a, b, a^2, ab, b^2]となる。polynomial_featuresがTrueでない場合は無視されます。|
|trigonometry_features: bool,<br>default = False|When set to True, new features are derived using existing numeric features.|Trueに設定すると、新しい特徴は既存の数値特徴を使って導き出される。|
|polynomial_threshold: float,<br>default = 0.1|When polynomial_features or trigonometry_features is True, new features are derived from the existing numeric features. This may sometimes result in too large feature space. polynomial_threshold parameter can be used to deal with this problem. It does so by using combination of Random Forest, AdaBoost and Linear correlation. All derived features that falls within the percentile distribution are kept and rest of the features are removed.|polynomial_featuresまたはtrigonometry_featuresがTrueの場合、新しいフィーチャーは既存の数値フィーチャーから派生します。polynomial_thresholdパラメータは、この問題に対処するために使用されます。polynomial_thresholdパラメータは、Random Forest、AdaBoost、Linear correlationを組み合わせて使用します。派生した特徴のうち、パーセンタイル分布に含まれるものはすべて残し、残りの特徴は削除されます。|
|group_features: list or list of list,<br>default = None|When the dataset contains features with related characteristics, group_features parameter can be used for feature extraction. It takes a list of strings with column names that are related.|データセットに関連性のある特徴が含まれている場合、group_featuresパラメータを特徴抽出に使用することができます。group_featuresは、関連性のあるカラム名を持つ文字列のリストを受け取ります。|
|group_names: list,<br>default = None|Group names to be used in naming new features. When the length of group_names does not match with the length of group_features, new features are named sequentially group_1, group_2, etc. It is ignored when group_features is None.|新しい特徴を命名する際に使用するグループ名です。group_namesの長さがgroup_featuresの長さと一致しない場合、新しい特徴はgroup_1、group_2などのように順次命名される。group_features が None の場合は無視される。|
|feature_selection: bool,<br>default = False| When set to True, a subset of features are selected using a combination of various permutation importance techniques including Random Forest, Adaboost and Linear correlation with target variable. The size of the subset is dependent on the feature_selection_threshold parameter.|Trueに設定すると、Random Forest、Adaboost、ターゲット変数との線形相関を含む様々な順列重要度技術の組み合わせを使用して、特徴のサブセットが選択される。サブセットのサイズは、feature_selection_threshold パラメータに依存します。|
|feature_selection_threshold: float,<br>default = 0.8|Threshold value used for feature selection. When polynomial_features or feature_interaction is True, it is recommended to keep the threshold low to avoid large feature spaces. Setting a very low value may be efficient but could result in under-fitting.|特徴選択に使われる閾値。polynomial_features または feature_interaction が True の場合、大きな特徴空間を避けるために、閾値を低く保つことが推奨される。非常に低い値を設定すると、効率的ではあるが、アンダーフィッティングになる可能性がある。|
|feature_selection_method: str, default = 'classic'|Algorithm for feature selection. 'classic' method uses permutation feature importance techniques. Other possible value is 'boruta' which uses boruta algorithm for feature selection.|特徴選択のアルゴリズムを指定する。classic」は順列特徴重要度法を用いる。その他の値として、'boruta'があり、これは特徴選択にborutaアルゴリズムを使用する。|
|feature_interaction: bool,<br>default = False|When set to True, new features are created by interacting (a * b) all the numeric variables in the dataset. This feature is not scalable and may not work as expected on datasets with large feature space.|Trueに設定すると、データセット内の全ての数値変数を相互作用（a * b）させることにより、新しい特徴が作成される。この機能はスケーラブルではなく、大きな特徴空間を持つデータセットでは期待通りに動作しない可能性がある。|
|feature_ratio: bool,<br>default = False|When set to True, new features are created by calculating the ratios (a / b) between all numeric variables in the dataset. This feature is not scalable and may not work as expected on datasets with large feature space.|Trueに設定すると、データセット内の全ての数値変数の間の比率（a / b）を計算することにより、新しい特徴を作成する。この機能はスケーラブルではなく、大きな特徴空間を持つデータセットでは期待通りに動作しない可能性がある。|
|interaction_threshold: bool,<br>default = 0.01|Similar to polynomial_threshold, It is used to compress a sparse matrix of newly created features through interaction. Features whose importance based on the combination of Random Forest, AdaBoost and Linear correlation falls within the percentile of the defined threshold are kept in the dataset. Remaining features are dropped before further processing.|polynomial_threshold と同様に、新たに作成された特徴の疎行列を相互作用によって圧縮するために使用される。ランダムフォレスト、AdaBoost、線形相関の組み合わせに基づく重要度が、定義された閾値のパーセンタイル内に収まる特徴は、データセットに残される。残りの特徴は、さらなる処理の前に削除される。|
|transform_target: bool,<br>default = False|When set to True, target variable is transformed using the method defined in transform_target_method param. Target transformation is applied separately from feature transformations.|Trueに設定された場合、transform_target_methodパラメータで定義された方法を用いて、ターゲット変数が変換される。ターゲットの変換は、フィーチャーの変換とは別に適用される。|
|transform_target_method: str,<br>default = 'box-cox'|'Box-cox' and 'yeo-johnson' methods are supported. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data. When transform_target_method is 'box-cox' and target variable contains negative values, method is internally forced to 'yeo-johnson' to avoid exceptions.| 'Box-cox' と 'yeo-johnson' のメソッドがサポートされています。Box-Coxは入力データが厳密に正であることを必要としますが、Yeo-Johnsonは正負両方のデータをサポートします。transform_target_methodが'box-cox'で、ターゲット変数に負の値が含まれている場合、例外を避けるためにメソッドは内部的に'yeo-johnson'に強制される。|
|data_split_shuffle: bool,<br>default = True|When set to False, prevents shuffling of rows during 'train_test_split'.|Falseに設定すると、'train_test_split'の際に行のシャッフルを行わない。|
|data_split_stratify: bool or list,<br>default = False|Controls stratification during 'train_test_split'. When set to True, will stratify by target column. To stratify on any other columns, pass a list of column names. Ignored when data_split_shuffle is False.|train_test_split'の際の層別を制御する。Trueに設定すると、対象となる列で層化します。他のカラムで層別するには、カラム名のリストを渡します。data_split_shuffleがFalseの場合は無視される。|
|fold_strategy: str or sklearn CV generator object,<br>default = 'kfold'|Choice of cross validation strategy. Possible values are:<br>        'kfold'<br>        'stratifiedkfold'<br>        'groupkfold'<br>        'timeseries'<br>        a custom CV generator object compatible with scikit-learn.|fold_strategy: str または sklearn CVジェネレータオブジェクト, default = 'kfold'.<br>    クロスバリデーション戦略の選択。可能な値は<br>        'kfold'<br>        'stratifiedkfold'<br>        'groupkfold'<br>        'timeseries'<br>        scikit-learnと互換性のあるカスタムCVジェネレータオブジェクト．|
|fold: int,<br>default = 10|Number of folds to be used in cross validation. Must be at least 2. This is a global setting that can be over-written at function level by using fold parameter. Ignored when fold_strategy is a custom object.|クロスバリデーションに使用するフォールドの数．これはグローバルな設定で，関数レベルでは fold パラメータを使って上書きすることができます．fold_strategy がカスタムオブジェクトの場合は無視されます。|
|fold_shuffle: bool,<br>default = False|ontrols the shuffle parameter of CV. Only applicable when fold_strategy is 'kfold' or 'stratifiedkfold'. Ignored when fold_strategy is a custom object.|CVのシャッフルパラメータをコントロールします。fold_strategyが'kfold'または'stratifiedkfold'の場合のみ適用可能。fold_strategyがカスタムオブジェクトの場合は無視されます。|
|fold_groups: str or array-like, with shape (n_samples,),<br>default = None|Optional group labels when 'GroupKFold' is used for the cross validation. It takes an array with shape (n_samples, ) where n_samples is the number of rows in the training dataset. When string is passed, it is interpreted as the column name in the dataset containing group labels.|GroupKFold'がクロスバリデーションに使われるときのオプションのグループラベル。これは，形状が(n_samples, )の配列をとります。ここで，n_samplesはトレーニングデータセットの行数です。stringが渡された場合、それはグループラベルを含むデータセットの列名として解釈されます。|
|n_jobs: int,<br>default = -1|The number of jobs to run in parallel (for functions that supports parallel processing) -1 means using all processors. To run all functions on single processor set n_jobs to None.|並列で実行するジョブの数（並列処理をサポートする関数の場合） -1は，すべてのプロセッサを使用することを意味します．すべての関数を単一のプロセッサで実行するには，n_jobs を None に設定する．|
|use_gpu: bool or str,<br>default = False|When set to True, it will use GPU for training with algorithms that support it, and fall back to CPU if they are unavailable. When set to 'force', it will only use GPU-enabled algorithms and raise exceptions when they are unavailable. When False, all algorithms are trained using CPU only.<br>GPU enabled algorithms:<br>        Extreme Gradient Boosting, requires no further installation<br>        CatBoost Regressor, requires no further installation (GPU is only enabled when data > 50,000 rows)<br>        Light Gradient Boosting Machine, requires GPU installation https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html<br>        Linear Regression, Lasso Regression, Ridge Regression, K Neighbors Regressor, Random Forest, Support Vector Regression, Elastic Net requires cuML >= 0.15 https://github.com/rapidsai/cuml| Trueに設定すると、GPUをサポートしているアルゴリズムの学習にGPUを使用し、使用できない場合はCPUにフォールバックします。force」に設定すると、GPUに対応したアルゴリズムのみを使用し、利用できない場合は例外を発生させます。False」に設定すると、すべてのアルゴリズムをCPUのみで学習します。<br>GPU対応のアルゴリズム。<br>        Extreme Gradient Boosting：追加のインストールは不要<br>        CatBoost Regressor：追加のインストールは必要ありません（GPUはデータが50,000行以上の場合のみ有効です<br>        Light Gradient Boosting Machine, GPUのインストールが必要 https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html<br>        Linear Regression, Lasso Regression, Ridge Regression, K Neighbors Regressor, Random Forest, Support Vector Regression, Elastic Net requires cuML >= 0.15 https://github.com/rapidsai/cuml|
|custom_pipeline: (str, transformer) or list of (str, transformer),<br>default = None|When passed, will append the custom transformers in the preprocessing pipeline and are applied on each CV fold separately and on the final fit. All the custom transformations are applied after 'train_test_split' and before pycaret's internal transformations.|渡された場合、前処理パイプラインにカスタムトランスフォーマーが追加され、各CVフォールドに個別に適用され、最終的なフィットにも適用されます。すべてのカスタム変換は，'train_test_split' の後，pycaret の内部変換の前に適用されます．|
|html: bool,<br>default = True|When set to False, prevents runtime display of monitor. This must be set to False when the environment does not support IPython. For example, command line terminal, Databricks Notebook, Spyder and other similar IDEs.|Falseに設定すると、モニタのランタイム表示を防ぎます。環境がIPythonをサポートしていない場合はFalseに設定する必要があります。例えば、コマンドラインターミナル、Databricks Notebook、Spyder、その他類似のIDEなどです。|
|session_id: int,<br>default = None|Controls the randomness of experiment. It is equivalent to 'random_state' in scikit-learn. When None, a pseudo random number is generated. This can be used for later reproducibility of the entire experiment.|実験のランダム性をコントロールします。scikit-learnの'random_state'に相当します。Noneの場合、疑似乱数が生成されます。これは、後に実験全体の再現性を高めるために使用できる。|
|log_experiment: bool,<br>default = False|When set to True, all metrics and parameters are logged on the MLFlow server.|Trueに設定すると、すべてのメトリクスとパラメータがMLFlowサーバに記録される。|
|experiment_name: str,<br>default = None|Name of the experiment for logging. Ignored when log_experiment is not True.|ロギングする実験の名前。log_experimentがTrueでない場合は無視される。|
|log_plots: bool or list,<br>default = False|When set to True, certain plots are logged automatically in the MLFlow server. To change the type of plots to be logged, pass a list containing plot IDs. Refer to documentation of plot_model. Ignored when log_experiment is not True.|Trueに設定すると、特定のプロットがMLFlowサーバに自動的に記録される。ログに記録するプロットの種類を変更するには、プロットIDを含むリストを渡す。plot_modelのドキュメントを参照してください。log_experimentがTrueでない場合は無視される。|
|log_profile: bool,<br>default = False|When set to True, data profile is logged on the MLflow server as a html file. Ignored when log_experiment is not True.|Log_profile： bool, default = False Trueに設定すると、データプロファイルがhtmlファイルとしてMLflowサーバーに記録されます。log_experimentがTrueでない場合は無視される。|
|log_data: bool,<br>default = False|When set to True, dataset is logged on the MLflow server as a csv file. Ignored when log_experiment is not True.|rueに設定すると、データセットがcsvファイルとしてMLflowサーバーに記録されます。log_experimentがTrueでない場合は無視される。|
|silent: bool,<br>default = False|Controls the confirmation input of data types when setup is executed. When executing in completely automated mode or on a remote kernel, this must be True.|セットアップ実行時のデータタイプの確認入力を制御する。完全に自動化されたモードで実行する場合や、リモートカーネル上で実行する場合は、Trueにする必要があります。|
|verbose: bool,<br>default = True| When set to False, Information grid is not printed.|Falseに設定すると、Information gridは印刷されない。|
|profile: bool,<br>default = False|When set to True, an interactive EDA report is displayed.|True に設定すると、インタラクティブな EDA レポートが表示されます。|
|profile_kwargs: dict,<br>default = {} (empty dict)|Dictionary of arguments passed to the ProfileReport method used to create the EDA report. Ignored if profile is False.|EDAレポートの作成に使用されるProfileReportメソッドに渡される引数の辞書です。profileがFalseの場合は無視されます|
|Returns|Global variables that can be changed using the set_config function.|set_config関数で変更可能なグローバル変数です。|
