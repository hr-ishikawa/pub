{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed084ee8-50a8-4b00-af81-43a2c252fabf",
   "metadata": {},
   "source": [
    "## OpenAI LLM (gpt) APIの利用\n",
    "\n",
    "https://developers.openai.com/api/reference/python/resources/responses/methods/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc96a146-5872-40c0-8ba7-e6a7117a7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13 13:15:59\n",
      "3.13.9 (tags/v3.13.9:8183fa5, Oct 14 2025, 14:09:13) [MSC v.1944 64 bit (AMD64)]\n",
      "openai = 2.16.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "import openai\n",
    "\n",
    "print(datetime.now().strftime('%F %X'))\n",
    "print(sys.version)\n",
    "print('openai =', version('openai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20557a80-3e93-49ed-bf50-d6684481a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL=gpt-5-nano\n",
      "\n",
      "以下は OpenAI の GPT 系列の主要モデルと、その特徴の要点です。時期や公開情報に基づく一般的な理解をまとめています。\n",
      "\n",
      "- GPT-1 (2018)\n",
      "  - アーキテクチャ: デコーダーのみの Transformer。約12層。\n",
      "  - パラメータ数: 約1.17億。\n",
      "  - 学習データ: BooksCorpus などの自己教師ありデータ。\n",
      "  - 主な特徴: 大規模事前学習による汎用性の示唆。微調整なしでも下流タスクでの性能が向上する可能性を初めて実証。\n",
      "  - 重要点: 以降のモデルの前提となる “pretraining + fine-tuning” の考え方の基礎。\n",
      "\n",
      "- GPT-2 (2019)\n",
      "  - アーキテクチャ: デコーダーのみの Transformer。最大 48 層、最大パラメータ約 1.5B。\n",
      "  - 学習データ: WebText（インターネット上の大規模テキスト）。\n",
      "  - 主な特徴: ゼロショット/少数ショットでの長文生成能力が顕著。自然言語生成の品質が大幅に向上。\n",
      "  - 重要点: 生成モデルとしてのリスク（誤情報・悪用の懸念）から段階的公開の経緯。\n",
      "\n",
      "- GPT-3 (2020)\n",
      "  - アーキテクチャ: デコーダーのみの Transformer。最大 175B パラメータ。\n",
      "  - 学習データ: 大規模なウェブデータの混合集合（Common Crawl など、複数ソース）。\n",
      "  - 主な特徴: 驚異的な Few-shot / in-context learning 能力。エンジン群（通常は ada, babbage, curie, davinci）として API 経由で利用可能。\n",
      "  - 重要点: 大規模化による汎用性の向上と、依然として事実性の誤認や偏りのリスクあり。\n",
      "\n",
      "- GPT-3.5 (2022–2023)\n",
      "  - アーキテクチャ: GPT-3 系列を基礎に、データ量・学習手法を強化。パラメータ数は公表されず、実質的には GPT-3 を超える性能向上。\n",
      "  - 学習手法: RLHF（強化学習による人間のフィードバック）を導入して指示遵守性を向上。\n",
      "  - 主な特徴: 指示に従う能力・会話の一貫性・安全性の改善。ChatGPT の基盤となるモデル群。\n",
      "  - 重要点: ユーザー意図に対する適合性は向上する一方、誤情報や境界事例の扱いには注意が必要。\n",
      "\n",
      "- GPT-4 (2023)\n",
      "  - アーキテクチャ: デコーダー系の大規模モデル。パラメータ数は公表されず。\n",
      "  - マルチモーダル: テキストだけでなく画像入力も処理可能（公式にマルチモーダル対応を宣言）。\n",
      "  - 学習データ: より多様・最新のデータで訓練とされるが、具体量は非公開。\n",
      "  - 主な特徴: 推論能力・長文理解・創造性・指示遵守性が大幅に向上。複雑な推論・多段階の計画・長文タスクでの性能改善。\n",
      "  - 安全性と信頼性: RLHF による指示適合性の改善、出力の安定性・偏りの低減を狙う。\n",
      "  - 追加バリアント: GPT-4 Turbo（高効率化・低コスト化された派生）。 context window の拡張オプションなど。\n",
      "  - 実務用途例: 複雑な文章作成、要約・翻訳、技術的な質問応答、画像を含む理解・説明、ツール呼び出し・プラグイン連携の基盤としての活用。\n",
      "\n",
      "- GPT-4o などの派生\n",
      "  - 内容: 音声入力/出力など追加モダリティを持つ派生モデルが公開されることがあり、対話の幅を拡張。\n",
      "  - 注意点: 詳細は公開情報に依存するため、公式の資料を都度確認してください。\n",
      "\n",
      "補足：モデル間の共通点と留意点\n",
      "- 共通点\n",
      "  - すべて「デコーダーのみの Transformer」を基盤とする点が基本設計。\n",
      "  - 大規模な自己教師データで事前学習し、下流タスクに適用するという設計思想は継承。\n",
      "- 異なる点\n",
      "  - パラメータ規模とデータ量が大きくなると、ゼロショット・少数ショットの能力が向上する一方、計算コストと安全/信頼性の課題は増大。\n",
      "  - GPT-4 以降はマルチモーダル対応とツール連携（関数呼び出し・プラグイン等）といった機能が追加される場合がある。\n",
      "\n",
      "もし用途別のおすすめや、特定のバージョンの実務での使い方（APIの使い方、プロンプト設計のコツ、リスク対策など）を詳しく知りたい場合は教えてください。用途に合わせて具体的な運用ガイドもご用意します。\n"
     ]
    }
   ],
   "source": [
    "# プロンプトを定義\n",
    "prompt = 'OpenAIのgptモデルシリーズの各モデル毎の特徴を教えてください'\n",
    "\n",
    "# モデルを設定\n",
    "#OPENAI_LLM_MODEL = 'gpt-5.2'\n",
    "#OPENAI_LLM_MODEL = 'gpt-5.1'\n",
    "#OPENAI_LLM_MODEL = 'gpt-5-mini'\n",
    "OPENAI_LLM_MODEL = 'gpt-5-nano'\n",
    "print(f\"MODEL={OPENAI_LLM_MODEL}\")\n",
    "\n",
    "# OpenAIクライアントを作成\n",
    "OPENAI_CLIENT = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# モデルを使用してレスポンスを生成\n",
    "\n",
    "response = OPENAI_CLIENT.responses.create(\n",
    "    model=OPENAI_LLM_MODEL,\n",
    "    input=[\n",
    "        {'role': 'system', 'content': [{'type': 'input_text', 'text': 'あなたは優秀な機械学習エンジニアです。質問に正確に答えてください。'}]},\n",
    "        {'role': 'user',   'content': [{'type': 'input_text', 'text': prompt}]}\n",
    "    ],\n",
    "#    temperature=0.2,  # 文書化の振れ幅 (Default:1.0, gpt-5-nano/miniでは利用不可)\n",
    ")\n",
    "\n",
    "# 応答テキストの取得\n",
    "print(f\"\\n{response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3ce54-b518-4f66-913a-fc337d182351",
   "metadata": {},
   "source": [
    "#### response内容\n",
    "```\n",
    "Response(\n",
    "    id='resp_03387d555595a36500698e97087930819697ef4dfd94b98189',\n",
    "    created_at=1770952456.0, error=None, incomplete_details=None, \n",
    "    instructions=None, \n",
    "    metadata={}, \n",
    "    model='gpt-5-nano-2025-08-07', \n",
    "    object='response', \n",
    "    output=[\n",
    "        ResponseReasoningItem(\n",
    "            id='rs_03387d555595a36500698e9708d5608196bdf717615eed8d0c', \n",
    "            summary=[], \n",
    "            type='reasoning', \n",
    "            content=None, \n",
    "            encrypted_content=None, \n",
    "            status=None\n",
    "        ), \n",
    "        ResponseOutputMessage(\n",
    "            id='msg_03387d555595a36500698e9722cf3c8196b05196863af7decd', \n",
    "            content=[\n",
    "                ResponseOutputText(\n",
    "                    annotations=[], \n",
    "                    text='以下は OpenAI の GPT 系列の主なモデルと、...', \n",
    "                    type='output_text', \n",
    "                    logprobs=[]\n",
    "                )\n",
    "            ], \n",
    "            role='assistant', \n",
    "            status='completed', \n",
    "            type='message'\n",
    "        )\n",
    "    ], \n",
    "    parallel_tool_calls=True, \n",
    "    temperature=1.0, \n",
    "    tool_choice='auto', \n",
    "    tools=[], \n",
    "    top_p=1.0, \n",
    "    background=False, \n",
    "    completed_at=1770952493.0, \n",
    "    conversation=None, \n",
    "    max_output_tokens=None, \n",
    "    max_tool_calls=None, \n",
    "    previous_response_id=None, \n",
    "    prompt=None, \n",
    "    prompt_cache_key=None, \n",
    "    prompt_cache_retention=None, \n",
    "    reasoning=Reasoning(\n",
    "        effort='medium', \n",
    "        generate_summary=None, \n",
    "        summary=None\n",
    "    ), \n",
    "    safety_identifier=None, \n",
    "    service_tier='default', \n",
    "    status='completed', \n",
    "    text=ResponseTextConfig(\n",
    "        format=ResponseFormatText(type='text'), \n",
    "        verbosity='medium'\n",
    "    ), \n",
    "    top_logprobs=0, \n",
    "    truncation='disabled', \n",
    "    usage=ResponseUsage(\n",
    "        input_tokens=52, \n",
    "        input_tokens_details=InputTokensDetails(cached_tokens=0), \n",
    "        output_tokens=5317, \n",
    "        output_tokens_details=OutputTokensDetails(reasoning_tokens=3776), \n",
    "        total_tokens=5369\n",
    "    ), \n",
    "    user=None, \n",
    "    billing={'payer': 'developer'}, \n",
    "    frequency_penalty=0.0, \n",
    "    presence_penalty=0.0, \n",
    "    store=True\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
