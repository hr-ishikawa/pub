table学習モデル、
- FT-Transformer
- TabTransformer
- TabNet
- TabPFN

のベースとするモデル、アーキテクチャ、category freatureおよび欠損値への対応などの特徴、Regression、Classificatinに対する特性、評価を


以下では、近年注目されている **テーブルデータ向けディープラーニングモデル** のうち、特に **トランスフォーマー系や近しい機構を取り入れた** 

- **FT-Transformer**  
- **TabTransformer**  
- **TabNet**  
- **TabPFN**  

について、それぞれ

1. **ベースとなるモデル・アーキテクチャ**  
2. **カテゴリ特徴量 (Categorical Feature) への対応**  
3. **欠損値 (Missing Values) への対応**  
4. **Regression (回帰) / Classification (分類) タスクでの特性・評価**  

という観点で整理します。

---

## 1. FT-Transformer

### (1) ベースモデル・アーキテクチャ

- **Transformer (Vaswani et al., 2017)** をテーブルデータに直接適用する発想  
- 各特徴量を「トークン」と見立て、トランスフォーマーが持つ **Multi-head Attention** 構造を通して、特徴量間の非線形な相互作用を学習  
- 基本的にはシンプルな構成で、**既存のTransformerのブロック構造** (Self-Attention, Feed Forward, Layer Normなど) を踏襲

### (2) カテゴリ特徴量への対応

- 数値・カテゴリを **統一的に“埋め込み(Embedding)”** してトークン化  
- カテゴリ特徴量は学習可能な埋め込みベクトルに変換し、数値特徴量も同様に埋め込みを施すことで、同一のトランスフォーマーレイヤに入力  
- ワンホットやターゲットエンコード等の前処理は原則不要（ただしカテゴリ数が異常に多い場合は工夫が必要）

### (3) 欠損値への対応

- 標準的には、**欠損値を埋め込むための特殊トークン** を定義するか、あるいは数値の場合は **0埋めや平均値埋め + 欠損フラグ** を特徴量として追加する方法がとられる  
- 「決定木系」ほど自然に欠損値を扱えるわけではなく、**前処理** が必要になるケースが多い  
  - 例: 欠損値を特殊カテゴリとして扱い埋め込む、など

### (4) Regression / Classification 特性・評価

- **Regression**  
  - 非線形の特徴量間相互作用を学習できるため、大規模・複雑な回帰タスクでも高い表現力が期待される  
  - 従来のDeep Learningより良好、決定木系(XGBoost等)と拮抗する・優位になる事例も報告されている
- **Classification**  
  - 数値＋カテゴリ混在の一般的な表形式データに広く適用可能  
  - 比較的ハイパーパラメータ設定がシンプルで、**スケーラビリティが高い** 点も評価されている

---

## 2. TabTransformer

### (1) ベースモデル・アーキテクチャ

- Transformerベースで **カテゴリ特徴量間の相互作用** を特に重視した構造  
- 「カテゴリ埋め込み層」をまとめて入力し、その組み合わせを **Multi-head Attention** で学習  
- 数値特徴量は別ルートで処理し、最後に融合(Fusion Layer)する設計

### (2) カテゴリ特徴量への対応

- **カテゴリ変数をすべて埋め込み(embedding)** して入力  
- 複数のカテゴリ特徴量の組み合わせ(相互作用)を注意機構で表現できるため、  
  従来のワンホットエンコードやターゲットエンコード以上に**高次元なカテゴリ関係をモデル化** 可能  
- カテゴリ次元が膨大でも、**埋め込みベクトルを使うため次元爆発を抑えられる**

### (3) 欠損値への対応

- 基本的にはFT-Transformer同様、**カテゴリ欠損の場合は“欠損カテゴリ”** として埋め込みを設定し、数値欠損は 0埋めや平均埋めなどを行う  
- TabTransformerとして独自に欠損値を自然に処理する仕組みは特にないため、**前処理** で対応する必要がある

### (4) Regression / Classification 特性・評価

- **Regression**  
  - 数値よりもカテゴリデータが分析の鍵となる回帰問題 (例: 顧客属性が重要な売上予測など) で効果的  
  - 逆に数値がメインでカテゴリが少ない場合、決定木系やFT-Transformerと大差ないこともある
- **Classification**  
  - **カテゴリ特徴量が多い** ケースで非常に高い精度を示す報告が多い  
  - 特に「複数カテゴリ属性の組み合わせ関係」が重要なタスク(顧客分析、マーケティング、レコメンデーションなど)で有用

---

## 3. TabNet

### (1) ベースモデル・アーキテクチャ

- **逐次的な特徴量選択 + アテンション機構** で構成された独自モデル  
- ネットワークが複数ステップにわたり入力特徴量を「どれだけ読むか(アテンションするか)」を学習する  
- **Gated Linear Unit (GLU)**、**sparsemax** といった仕組みを用い、深層学習ながらも **解釈性をある程度確保** できるよう工夫

### (2) カテゴリ特徴量への対応

- 数値/カテゴリともに**埋め込み**を行い、各ステップのアテンション機構を通して学習  
- カテゴリ埋め込みの方法は実装によって異なるが、一般にワンホットや学習可能埋め込みで変換後、TabNetのネットワークに入力  
- ただし、**TabTransformerほどカテゴリ特化** しているわけではなく、あくまで「重要度を段階的に学習」するアプローチ

### (3) 欠損値への対応

- 特別に欠損値処理ロジックが組み込まれているわけではないため、**あらかじめ前処理が必要**  
- 欠損が多い場合は、One-hotで欠損フラグを追加する、あるいは埋め込みで「欠損専用カテゴリ」を持たせるなどの対処が一般的  
- 決定木系のように「自然に欠損を分岐する」仕組みはない

### (4) Regression / Classification 特性・評価

- **Regression**  
  - 重要な特徴量を段階的に選択するため、**ノイズが多い** データセットでも比較的安定した性能報告がある  
  - ただし、モデル構造が複雑で学習に時間がかかり、大規模データではハイパーチューニングも含め負荷が大きい
- **Classification**  
  - 決定木系 (LightGBM / XGBoost) と同等以上の精度を得られるとする研究事例多数  
  - 「どの特徴量が重要だったか」を可視化しやすく、**ビジネス現場での説明性** を求められるケースにフィット

---

## 4. TabPFN (Prior-Functional Neural Networks)

### (1) ベースモデル・アーキテクチャ

- **ベイズ的アプローチ + メタ学習** を組み合わせ、事前に大規模シミュレーションで「学習済みのモデル (Prior)」を作成  
- 新規のタスクに対しては、**ほぼハイパーパラメータ調整不要** かつ **数秒で学習・推論** できるのが最大の特徴  
- Transformerを直接ベースにしているわけではないが、**自己注意(Self-Attention)的な要素** を内部に含む

### (2) カテゴリ特徴量への対応

- 小〜中規模データを想定しているため、カテゴリ特徴量の次元が極端に大きくなければ **通常の埋め込み (or One-hot)** で問題なく対応可能  
- 大量カテゴリ / 多階層カテゴリなどには未対応部分も多く、**実装や前処理で補う** 必要がある

### (3) 欠損値への対応

- TabPFN自体には **専用の欠損値処理** は備わっていない  
- 一般的には欠損を埋める or 欠損フラグを追加して扱うなど、**前処理が必要**  
- 小データの場合、欠損の扱いが難しいケースもあるが、周辺確率を推定するベイズ的アプローチにより、ある程度頑健性が期待されるとの報告もある

### (4) Regression / Classification 特性・評価

- **Regression**  
  - **小規模データ** (数百〜数千サンプル) で数秒以内に高精度を出せる例が多い  
  - 大規模 / 高度に非線形な回帰にはまだ限界があり、適用事例は限定的
- **Classification**  
  - **小データ** での分類性能が特筆されており、**最先端モデルと同等 or 上回る** 報告もあり  
  - 多クラスにも対応可能で、推論速度が圧倒的に速いのがメリット  
  - ほぼハイパーパラメータ不要なので、データサイエンティストの「手間を減らす」面でも評価が高い

---

## 全体比較表

| モデル              | ベース / アーキテクチャ                                                       | カテゴリ特徴量への対応                                                    | 欠損値への対応                 | Regression特性                                        | Classification特性                                  |
|---------------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------|----------------------------|--------------------------------------------------|-------------------------------------------------|
| **FT-Transformer** | - Transformer (Feature Token) <br>- シンプル構造                                | - 数値/カテゴリを同様に埋め込み <br>- ワンホット不要                 | - 特殊トークンや前処理が必要(0埋め, 欠損フラグ等) | - 大規模でもスケーラブル <br>- 非線形相互作用が強み              | - 決定木系と同等以上 <br>- 汎用性とチューニング容易性が魅力          |
| **TabTransformer** | - Transformer (カテゴリ埋め込み特化)                                         | - カテゴリ埋め込み + Attention <br>- 多次元カテゴリでも次元爆発を抑制 | - 前処理で欠損カテゴリとして扱う等が必要              | - カテゴリ主導型の回帰に強み <br>- 数値主体では差が出にくい         | - カテゴリ多数のタスクで高精度 <br>- 従来のエンコードより高表現力       |
| **TabNet**         | - 独自アーキテクチャ (逐次的アテンション + 特徴量選択) <br>- GLU, sparsemaxなど  | - 埋め込み + 逐次的アテンション <br>- 特徴量重要度可視化が容易          | - 特別な処理はないため前処理依存                | - ノイズに強く安定 <br>- 解釈性が高くビジネス利用に向く            | - 決定木系に匹敵または超える報告 <br>- 解釈性が求められる分類で評価高い  |
| **TabPFN**         | - メタ学習 + ベイズ的アプローチ <br>- 事前学習済みの “Prior” を利用             | - 小〜中規模データなら簡易埋め込み <br>- 大量カテゴリには未対応部分あり | - 特別な欠損処理は備わらないため前処理必須          | - 小規模回帰で超高速かつ高精度 <br>- 大規模回帰は苦手               | - 小データ分類で最先端並み <br>- ハイパーパラメータ不要 & 超高速        |

---

## まとめと選択のポイント

1. **カテゴリ特徴量が大量にある**  
   - **TabTransformer** や **FT-Transformer** は埋め込み + Attentionで効率的に学習  
   - **TabNet** も対応可能だが、さらに可視化や解釈性を重視するなら有力

2. **欠損値が多い**  
   - いずれのモデルも、**決定木系** のように自然に欠損処理を組み込んでいるわけではない  
   - 欠損値を特殊トークン化 / 0埋め & 欠損フラグ追加 / “欠損カテゴリ” などの前処理が必要になる

3. **小規模データ** かつ **素早いモデリング**  
   - **TabPFN** が有力 (ほぼハイパーパラメータ調整不要、数秒で学習完了)  
   - 一方、大規模データには向かないため注意

4. **大規模 & 多様な特徴量**  
   - **FT-Transformer** や **TabTransformer** が有力候補  
   - GPU/TPU等のリソースが用意できれば、Transformer機構で高い表現力を引き出せる

5. **解釈性が重要 (モデルの“中身”を説明したい)**  
   - **TabNet** は逐次的アテンションによる特徴量重要度可視化が比較的容易  
   - トランスフォーマー系（FT / TabTransformer）は追加の可視化手法やアテンションマップ解析が必要になる

それぞれのモデルは、**従来の決定木系手法 (XGBoost, LightGBM等) とアンサンブル** することでさらに性能向上が見込まれる場合もあります。  
タスク要件（データ規模、カテゴリの多さ、欠損値の分布、必要な解釈性レベルなど）を総合的に考慮しながら、これらのモデルを使い分けるのが効果的です。
