table学習モデル、
- FT-Transformer
- TabTransformer
- TabNet
- TabPFN
のベースとするモデル、アーキテクチャ、category freatureへの対応などの特徴、Regression、Classificatinに対する特性、評価を


以下では、近年注目されている **テーブルデータ向けディープラーニングモデル** のうち、特にトランスフォーマーやそれに近い機構を取り入れている

- **FT-Transformer**  
- **TabTransformer**  
- **TabNet**  
- **TabPFN**  

について、それぞれ

1. **ベースとなるモデル・アーキテクチャ**  
2. **カテゴリ特徴量 (categorical features) への対応**  
3. **Regression (回帰) / Classification (分類) タスクでの特性・評価**  

という観点でまとめます。

---

## 1. FT-Transformer

### (1) ベースモデル・アーキテクチャ
- **Transformer (Vaswani et al.)** のエッセンスをテーブルデータに導入  
- 各特徴量を「トークン」と見なし、トランスフォーマーの **Multi-head Attention** と **Feed Forward** を通して学習
- 位置埋め込み (Positional Encoding) や Layer Normalization 等の標準的な仕組みを踏襲し、**シンプルな構成** で実装される

### (2) カテゴリ特徴量への対応
- 数値・カテゴリの区別なく、**すべての特徴量を統一的に埋め込み** (Embedding) トークンとして扱う  
- カテゴリ特徴量は学習可能な埋め込みベクトルに変換し、数値特徴量と同一のトランスフォーマーブロックへ入力  
  - 従来のワンホットエンコードやターゲットエンコードは不要

### (3) Regression / Classification 特性・評価
- **回帰 (Regression)**  
  - 非線形な特徴量間の相互作用を捉えられるため、複雑な回帰問題でも性能向上が期待される  
  - **大規模データセット** にもスケーラブルで、十分な計算リソースがあれば高い表現力を発揮
- **分類 (Classification)**  
  - 数値とカテゴリが混在する典型的なテーブルデータの分類でも、**競合する決定木系 (XGBoost等) と肩を並べる or 上回る** 報告もあり  
  - Transformerとしては比較的シンプルな構成なので、**ハイパーパラメータ調整も容易** というメリットがある

---

## 2. TabTransformer

### (1) ベースモデル・アーキテクチャ
- **Transformer** をベースにしつつ、**カテゴリ特徴量の埋め込みと相互作用** を特に重視したモデル  
- カテゴリ埋め込み層と数値特徴量を扱うルートを分離し、最終段階で両者を融合する構造

### (2) カテゴリ特徴量への対応
- **カテゴリ特徴量をすべてエンベディング (embedding) して入力**  
- 複数のカテゴリ特徴量間の **相互作用を Attention でモデル化** できるよう工夫  
- 従来のワンホットエンコード等に起因する次元爆発を避けながら、カテゴリ特徴量の組み合わせを学習できる  
  - カテゴリが多い、あるいは高次元にわたる場合に特に強みを発揮

### (3) Regression / Classification 特性・評価
- **回帰 (Regression)**  
  - メインが数値特徴量の場合は、**決定木系やFT-Transformer等と比較して明確な差が出づらい** という指摘もあり  
  - しかし、カテゴリ特徴量が回帰において重要な役割を持つ場合は、**高い精度** を示す事例あり
- **分類 (Classification)**  
  - 特に **カテゴリ特徴量が多いタスク** で成果を上げている  
  - マーケティングやユーザー行動分析など、多くのカテゴリ属性を含むデータセットで **従来手法を上回る** 精度報告がある

---

## 3. TabNet

### (1) ベースモデル・アーキテクチャ
- **逐次的な特徴量選択** と **アテンション機構** を組み合わせた **TabNet 独自の構造**  
- マルチステップのネットワークを通して、「どの特徴量にどの程度注意を払うか」を自動的に学習し、重要度を割り振る  
- **Gated Linear Units (GLU)** や **sparsemax** などを用いて、深層学習でありながら一部 **解釈可能性** を向上

### (2) カテゴリ特徴量への対応
- 数値・カテゴリともに、**エンベディング＋各ステップのアテンション** で取り扱い  
- TabTransformerほどカテゴリ特化ではないが、embedding層の工夫やスパースアテンションにより、**カテゴリ情報を活かしやすい**  
- 特徴量単位での寄与度が可視化しやすく、カテゴリ変数がどの程度効いているかを把握可能

### (3) Regression / Classification 特性・評価
- **回帰 (Regression)**  
  - 決定木系や他のディープモデルと比較し、**高い解釈性** が必要な現場では注目度が高い  
  - 特徴量の重要度が明示されるため、ノイズの多い環境下でも安定した性能を示すケースがある
- **分類 (Classification)**  
  - XGBoostやLightGBMなどの **決定木系手法と同等以上の精度** を得られる報告が多い  
  - マルチクラス分類にも対応しやすく、クラス不均衡データでもアテンション機構が強みを発揮する

---

## 4. TabPFN (Prior-Functional Neural Networks)

### (1) ベースモデル・アーキテクチャ
- **メタ学習** 的な発想と **ベイズ的アプローチ** を融合し、あらかじめ「学習済みモデル (Prior)」を構築  
- 新たなタスクに対しては、ほとんどハイパーパラメータ調整を要せず、**数秒レベルで推論** できる点が最大の特徴  
- Transformerを直接ベースとしているわけではないが、**自己注意 (Self-Attention) を利用した要素** も含まれており、「Prior-Functional Network」という独自の設計

### (2) カテゴリ特徴量への対応
- 一般的には、数値・カテゴリとも**埋め込み** や **適切なエンコーディング** をしたうえで TabPFN に入力  
- 小規模データを想定しているため、通常の **ワンホット or 埋め込み** で性能を確保可能  
- 大規模・高次元カテゴリ特徴量に対する特化機能はそこまで強調されていない

### (3) Regression / Classification 特性・評価
- **回帰 (Regression)**  
  - **小規模データ** (数百〜数千サンプル程度) の回帰問題なら、数秒以内で十分な性能を発揮するケースがある  
  - 大規模/高度に非線形な回帰タスクでは、まだ従来手法や他のディープモデルの方が優位な場合も
- **分類 (Classification)**  
  - **小データセット** 向けの分類問題で、最先端モデルと同等か、それ以上の報告もあり  
  - ほぼ **ハイパーパラメータ不要** という手軽さと、**高速推論** が最大の魅力  
  - Kaggleなどのコンペでも、小〜中規模のタスクで短時間に高スコアを狙う際に有力

---

## 特徴の総括

以下の表に、各モデルの **ベースアーキテクチャ・カテゴリ特徴量への対応・Regression/Classification特性** を簡単に比較します。

| モデル              | ベース / アーキテクチャ                                                                                      | カテゴリ特徴量への対応                                               | Regression特性                              | Classification特性                           |
|---------------------|---------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|----------------------------------------|----------------------------------------|
| **FT-Transformer** | - Transformer (Feature Token) <br>- シンプルな構造                                                                | - 数値/カテゴリを統一的に埋め込みトークン化                               | - 大規模・非線形回帰で強み <br>- チューニング容易             | - 決定木系と同等以上の精度も <br>- 広範な適用が可能               |
| **TabTransformer** | - Transformer <br>- カテゴリ特徴量の相互作用を強化                                                                    | - カテゴリを埋め込み + 注意機構 <br>- 多次元カテゴリにもスケーラブル             | - カテゴリが重要な回帰で力を発揮 <br>- 数値主体なら差が出にくい          | - カテゴリ多数の分類タスクで高精度 <br>- 従来エンコーディングよりも高い表現力 |
| **TabNet**         | - 独自アーキテクチャ (逐次的アテンション + 特徴量選択) <br>- Gated Linear Unit, sparsemax 等                                  | - 埋め込み + 逐次アテンション <br>- 特徴量重要度を可視化しやすい                | - ノイズにも比較的強く安定 <br>- 解釈性・可視化を重視する現場向き      | - 決定木系に匹敵または超える精度 <br>- 解釈性が比較的高い            |
| **TabPFN**         | - メタ学習 + ベイズ的手法 <br>- Prior-Functional Network (自己注意も要素として含む)                               | - 小規模データ前提で簡易な埋め込みが主流 <br>- 大規模・多次元カテゴリへの特化はやや弱い | - 小規模回帰で数秒で高精度 <br>- 大規模回帰は苦手             | - 小規模分類でトップクラス <br>- 超高速 & ハイパーパラメータ不要      |

### 選択のポイント
- **カテゴリ特徴量が大量にある** → **TabTransformer** や **FT-Transformer** が有力候補  
- **解釈性が欲しい** → **TabNet** (特徴量重要度の可視化がしやすい)  
- **小規模データで高速に結果を得たい** → **TabPFN** (ほぼハイパーパラメータ不要)  
- **大規模データ・多種多様な特徴量を取り扱う** → **FT-Transformer** や **TabTransformer** (Transformerベースの汎用性)  

いずれのモデルも、従来の決定木系手法 (XGBoost, LightGBM 等) とアンサンブルすることでさらに性能を上げられる場合があります。タスクの特性やデータ規模、解釈性の必要性、計算資源などを考慮しながら使い分けるのが重要です。


